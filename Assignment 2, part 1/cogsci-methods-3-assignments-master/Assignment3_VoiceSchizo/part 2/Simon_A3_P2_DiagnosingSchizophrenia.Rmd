---
title: "Assignment 3 - Part 2 - Diagnosing Schizophrenia from Voice"
author: "Simon MÃ¸ller Nielsen"
date: "28-10 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(lmerTest)
library(tidyverse)
library(caret)
library(boot)
library(ROCR)
library(pROC)

setwd("C:/Users/slmoni/Documents/Uni/Experimental Methods III/R-stuff/Assignment 2, part 1/cogsci-methods-3-assignments-master/Assignment3_VoiceSchizo/part 2")
```

## Assignment 3 - Diagnosing schizophrenia from voice

In the previous part of the assignment you generated a bunch of "features", that is, of quantitative descriptors of voice in schizophrenia, focusing on pitch.
In the course of this assignment we will use them to try to automatically diagnose schizophrenia from voice only, that is, relying on the set of features you produced last time, we will try to produce an automated classifier.

### Question 1: Can you diagnose schizophrenia from pitch range only? If so, how well?

Build a logistic regression to see whether you can diagnose schizophrenia from pitch range only.

```{r, include=FALSE}
# Loading data
pitchData<-read.csv("SimonPitchData.csv")

# Log transforming the data
pitchData$Mean<-log(pitchData$Mean)
pitchData$SD<-log(pitchData$SD)
pitchData$Range<-log(pitchData$Range)
pitchData$Median<-log(pitchData$Median)
pitchData$X5.<-log(pitchData$X5.)
pitchData$X25.<-log(pitchData$X25.)
pitchData$X75.<-log(pitchData$X75.)
pitchData$X95.<-log(pitchData$X95.)

#standardize the data to cohen's d
pitchData$Mean<-(pitchData$Mean-mean(pitchData$Mean))/sd(pitchData$Mean)
pitchData$SD<-(pitchData$SD-mean(pitchData$SD))/sd(pitchData$SD)
pitchData$Range<-(pitchData$Range-mean(pitchData$Range))/sd(pitchData$Range)
pitchData$Median<-(pitchData$Median-mean(pitchData$Median))/sd(pitchData$Median)
pitchData$X5.<-(pitchData$X5.-mean(pitchData$X5.))/sd(pitchData$X5.)
pitchData$X25.<-(pitchData$X25.-mean(pitchData$X25.))/sd(pitchData$X25.)
pitchData$X75.<-(pitchData$X75.-mean(pitchData$X75.))/sd(pitchData$X75.)
pitchData$X95.<-(pitchData$X95.-mean(pitchData$X95.))/sd(pitchData$X95.)

# Building simple model
simpleRange = glm(Diagnosis ~ Range, data= pitchData, family="binomial")
summary(simpleRange)
```

Calculate the different performance measures (accuracy, sensitivity, specificity, PPV, NPV, ROC curve) on a logistic regression using the full dataset. Don't forget the random effects!
```{r, include=FALSE}
# Building simple model
rangeModel = glmer(Diagnosis ~ Range + (1|Subject) + (1|Triangles), data= pitchData, family="binomial")
summary(rangeModel)

# Prediting data
pitchData$predictions <- inv.logit(predict(rangeModel, newdata = pitchData, allow.new.levels=TRUE))

pitchData$predictions[pitchData$predictions > 0.5] = "Schizophrenia"
pitchData$predictions[pitchData$predictions <= 0.5] = "Control"

# Checking the predictions
confusionMatrix(data = as.factor(pitchData$predictions), reference = as.factor(pitchData$Diagnosis), positive = "Schizophrenia")

# Making ROC curve
pitchData$predictions <- inv.logit(predict(rangeModel, newdata = pitchData, allow.new.levels=TRUE))
rocCurve <- roc(response = pitchData$Diagnosis, predictor= pitchData$predictions)
auc(rocCurve)
ci(rocCurve)
plot(rocCurve, legacy.axes=TRUE)
```

Then cross-validate the logistic regression and re-calculate performance on the testing folds. N.B. The cross-validation functions you already have should be tweaked: you need to calculate these new performance measures.

N.B. the predict() function generates log odds (the full scale between minus and plus infinity). Log odds > 0 indicates a choice of 1, below a choice of 0.
N.B. you need to decide whether calculate performance on each single test fold or save all the prediction for test folds in one dataset, so to calculate overall performance.
N.B. Now you have two levels of structure: subject and study. Should this impact your cross-validation?
```{r Range model, include=FALSE}
# Cross validation for the range model
Folds<-createFolds(unique(pitchData$Subject), k=5, list=T, returnTrain=F)
Folds<-lapply(Folds, function(x) unique(pitchData$Subject)[x])

n=0

predictors<- rep(NA, nrow(pitchData))

for (i in Folds) {
  n=n+1
  trainFold = filter(pitchData, !Subject %in% i)
  testFold = filter(pitchData, Subject %in% i)
  rangeModel = glmer(Diagnosis ~ Range + (1|Subject) + (1|Triangles), data= trainFold, family="binomial")
  
  pTest <- inv.logit(predict(rangeModel, newdata = testFold, allow.new.levels=TRUE))
  
  predictors[pitchData$Subject %in% i] = pTest
}

# Making ROC curve
rocCurve <- roc(response = pitchData$Diagnosis, predictor= predictors)
auc(rocCurve)
ci(rocCurve)
plot(rocCurve, legacy.axes=TRUE)

# Checking the predictions
predictors[predictors > 0.5] = "Schizophrenia"
predictors[predictors <= 0.5] = "Control"

confusionMatrix(data = as.factor(predictors), reference = as.factor(pitchData$Diagnosis), positive = "Schizophrenia")
```
### Question 2 - Which single acoustic predictor is the best predictor of diagnosis?
```{r Mean model, include=FALSE}
Folds<-createFolds(unique(pitchData$Subject), k=5, list=T, returnTrain=F)
Folds<-lapply(Folds, function(x) unique(pitchData$Subject)[x])

n=0

predictors<- rep(NA, nrow(pitchData))

for (i in Folds) {
  n=n+1
  trainFold = filter(pitchData, !Subject %in% i)
  testFold = filter(pitchData, Subject %in% i)
  rangeModel = glmer(Diagnosis ~ Mean + (1|Subject) + (1|Triangles), data= trainFold, family="binomial")
  
  pTest <- inv.logit(predict(rangeModel, newdata = testFold, allow.new.levels=TRUE))
  
  predictors[pitchData$Subject %in% i] = pTest
}

# Making ROC curve
rocCurve <- roc(response = pitchData$Diagnosis, predictor= predictors)
auc(rocCurve)
ci(rocCurve)
plot(rocCurve, legacy.axes=TRUE)

# Checking the predictions
predictors[predictors > 0.5] = "Schizophrenia"
predictors[predictors <= 0.5] = "Control"

confusionMatrix(data = as.factor(predictors), reference = as.factor(pitchData$Diagnosis), positive = "Schizophrenia")
```

```{r SD model, include=FALSE}
# Cross validation for the range model
Folds<-createFolds(unique(pitchData$Subject), k=5, list=T, returnTrain=F)
Folds<-lapply(Folds, function(x) unique(pitchData$Subject)[x])

n=0

predictors<- rep(NA, nrow(pitchData))

for (i in Folds) {
  n=n+1
  trainFold = filter(pitchData, !Subject %in% i)
  testFold = filter(pitchData, Subject %in% i)
  rangeModel = glmer(Diagnosis ~ SD + (1|Subject) + (1|Triangles), data= trainFold, family="binomial")
  
  pTest <- inv.logit(predict(rangeModel, newdata = testFold, allow.new.levels=TRUE))
  
  predictors[pitchData$Subject %in% i] = pTest
}

# Making ROC curve
rocCurve <- roc(response = pitchData$Diagnosis, predictor= predictors)
auc(rocCurve)
ci(rocCurve)
plot(rocCurve, legacy.axes=TRUE)

# Checking the predictions
predictors[predictors > 0.5] = "Schizophrenia"
predictors[predictors <= 0.5] = "Control"

confusionMatrix(data = as.factor(predictors), reference = as.factor(pitchData$Diagnosis), positive = "Schizophrenia")
```

```{r Median model, include=FALSE}
# Cross validation for the range model
Folds<-createFolds(unique(pitchData$Subject), k=5, list=T, returnTrain=F)
Folds<-lapply(Folds, function(x) unique(pitchData$Subject)[x])

n=0

predictors<- rep(NA, nrow(pitchData))

for (i in Folds) {
  n=n+1
  trainFold = filter(pitchData, !Subject %in% i)
  testFold = filter(pitchData, Subject %in% i)
  rangeModel = glmer(Diagnosis ~ Median + (1|Subject) + (1|Triangles), data= trainFold, family="binomial")
  
  pTest <- inv.logit(predict(rangeModel, newdata = testFold, allow.new.levels=TRUE))
  
  predictors[pitchData$Subject %in% i] = pTest
}

# Making ROC curve
rocCurve <- roc(response = pitchData$Diagnosis, predictor= predictors)
auc(rocCurve)
ci(rocCurve)
plot(rocCurve, legacy.axes=TRUE)

# Checking the predictions
predictors[predictors > 0.5] = "Schizophrenia"
predictors[predictors <= 0.5] = "Control"

confusionMatrix(data = as.factor(predictors), reference = as.factor(pitchData$Diagnosis), positive = "Schizophrenia")
```

```{r 0.05 quantial model, include=FALSE}
# Cross validation for the range model
Folds<-createFolds(unique(pitchData$Subject), k=5, list=T, returnTrain=F)
Folds<-lapply(Folds, function(x) unique(pitchData$Subject)[x])

n=0

predictors<- rep(NA, nrow(pitchData))

for (i in Folds) {
  n=n+1
  trainFold = filter(pitchData, !Subject %in% i)
  testFold = filter(pitchData, Subject %in% i)
  rangeModel = glmer(Diagnosis ~ X5. + (1|Subject) + (1|Triangles), data= trainFold, family="binomial")
  
  pTest <- inv.logit(predict(rangeModel, newdata = testFold, allow.new.levels=TRUE))
  
  predictors[pitchData$Subject %in% i] = pTest
}

# Making ROC curve
rocCurve <- roc(response = pitchData$Diagnosis, predictor= predictors)
auc(rocCurve)
ci(rocCurve)
plot(rocCurve, legacy.axes=TRUE)

# Checking the predictions
predictors[predictors > 0.5] = "Schizophrenia"
predictors[predictors <= 0.5] = "Control"

confusionMatrix(data = as.factor(predictors), reference = as.factor(pitchData$Diagnosis), positive = "Schizophrenia")
```

```{r 0.25 quantial model, include=FALSE}
# Cross validation for the range model
Folds<-createFolds(unique(pitchData$Subject), k=5, list=T, returnTrain=F)
Folds<-lapply(Folds, function(x) unique(pitchData$Subject)[x])

n=0

predictors<- rep(NA, nrow(pitchData))

for (i in Folds) {
  n=n+1
  trainFold = filter(pitchData, !Subject %in% i)
  testFold = filter(pitchData, Subject %in% i)
  rangeModel = glmer(Diagnosis ~ X25. + (1|Subject) + (1|Triangles), data= trainFold, family="binomial")
  
  pTest <- inv.logit(predict(rangeModel, newdata = testFold, allow.new.levels=TRUE))
  
  predictors[pitchData$Subject %in% i] = pTest
}

# Making ROC curve
rocCurve <- roc(response = pitchData$Diagnosis, predictor= predictors)
auc(rocCurve)
ci(rocCurve)
plot(rocCurve, legacy.axes=TRUE)

# Checking the predictions
predictors[predictors > 0.5] = "Schizophrenia"
predictors[predictors <= 0.5] = "Control"

confusionMatrix(data = as.factor(predictors), reference = as.factor(pitchData$Diagnosis), positive = "Schizophrenia")
```

```{r 0.75 quantial model, include=FALSE}
# Cross validation for the range model
Folds<-createFolds(unique(pitchData$Subject), k=5, list=T, returnTrain=F)
Folds<-lapply(Folds, function(x) unique(pitchData$Subject)[x])

n=0

predictors<- rep(NA, nrow(pitchData))

for (i in Folds) {
  n=n+1
  trainFold = filter(pitchData, !Subject %in% i)
  testFold = filter(pitchData, Subject %in% i)
  rangeModel = glmer(Diagnosis ~ X75. + (1|Subject) + (1|Triangles), data= trainFold, family="binomial")
  
  pTest <- inv.logit(predict(rangeModel, newdata = testFold, allow.new.levels=TRUE))
  
  predictors[pitchData$Subject %in% i] = pTest
}

# Making ROC curve
rocCurve <- roc(response = pitchData$Diagnosis, predictor= predictors)
auc(rocCurve)
ci(rocCurve)
plot(rocCurve, legacy.axes=TRUE)

# Checking the predictions
predictors[predictors > 0.5] = "Schizophrenia"
predictors[predictors <= 0.5] = "Control"

confusionMatrix(data = as.factor(predictors), reference = as.factor(pitchData$Diagnosis), positive = "Schizophrenia")
```

```{r 0.95 quantial model, include=FALSE}
# Cross validation for the range model
Folds<-createFolds(unique(pitchData$Subject), k=5, list=T, returnTrain=F)
Folds<-lapply(Folds, function(x) unique(pitchData$Subject)[x])

n=0

predictors<- rep(NA, nrow(pitchData))

for (i in Folds) {
  n=n+1
  trainFold = filter(pitchData, !Subject %in% i)
  testFold = filter(pitchData, Subject %in% i)
  rangeModel = glmer(Diagnosis ~ X95. + (1|Subject) + (1|Triangles), data= trainFold, family="binomial")
  
  pTest <- inv.logit(predict(rangeModel, newdata = testFold, allow.new.levels=TRUE))
  
  predictors[pitchData$Subject %in% i] = pTest
}

# Making ROC curve
rocCurve <- roc(response = pitchData$Diagnosis, predictor= predictors)
auc(rocCurve)
ci(rocCurve)
plot(rocCurve, legacy.axes=TRUE)

# Checking the predictions
predictors[predictors > 0.5] = "Schizophrenia"
predictors[predictors <= 0.5] = "Control"

confusionMatrix(data = as.factor(predictors), reference = as.factor(pitchData$Diagnosis), positive = "Schizophrenia")
```

### Question 3 - Which combination of acoustic predictors is best for diagnosing schizophrenia?

Now it's time to go wild! Use all (voice-related) variables and interactions you can think of. Compare models and select the best performing model you can find.

Remember:
- Out-of-sample error crucial to build the best model!
- After choosing the model, send Malte and Riccardo the code of your model
```{r Best model, include=FALSE}
# See Laurits Code for loop of the different models  
Folds<-createFolds(unique(pitchData$Subject), k=5, list=T, returnTrain=F)
Folds<-lapply(Folds, function(x) unique(pitchData$Subject)[x])

n=0

predictors<- rep(NA, nrow(pitchData))

for (i in Folds) {
  n=n+1
  trainFold = filter(pitchData, !Subject %in% i)
  testFold = filter(pitchData, Subject %in% i)
  rangeModel = glmer(Diagnosis ~ Range + Median + SD + (1|Subject) + (1|Triangles), data= trainFold, family="binomial")
  
  pTest <- inv.logit(predict(rangeModel, newdata = testFold, allow.new.levels=TRUE))
  
  predictors[pitchData$Subject %in% i] = pTest
}

# Making ROC curve
rocCurve <- roc(response = pitchData$Diagnosis, predictor= predictors)
auc(rocCurve)
ci(rocCurve)
plot(rocCurve, legacy.axes=TRUE)

# Checking the predictions
predictors[predictors > 0.5] = "Schizophrenia"
predictors[predictors <= 0.5] = "Control"

confusionMatrix(data = as.factor(predictors), reference = as.factor(pitchData$Diagnosis), positive = "Schizophrenia")
```

### Question 4: Properly report the results

METHODS SECTION: how did you analyse the data? That is, how did you extract the data, designed the models and compared their performance?

For the analysis of pitch 8 features were extracted, mean, standard deviation, range, 0.05 quantile, 0.25 quantile, median, 0.75 quantile and 0.95 quantile. A binomial  linear mixed effects model was created for each features as predictors of diagnosis. Random slops were added by subject and trial. Models were trained using 5 fold cross validation.
The models were assessed by looking at thier accuracy, sensitivity, specificity, PPV, NPV, ROC curve, at predicting the training fold. The cut off point was set to inverse logit 0.5, with equal or under being predicted as control and above as schizophrenic.

RESULTS SECTION: can you diagnose schizophrenia based on voice? which features are used? Comment on the difference between the different performance measures.

The preformance of the different predictores are as follows:
Predictor         Accuracy  Sensitivity   Specificity   PPV     NPV     Area under ROC curve
Mean              0.5034    0.4528        0.5549        0.5084  0.4993  0.5308
SD                0.5805    0.6342        0.5259        0.5763  0.5857  0.6041
Range             0.5797    0.6072        0.5518        0.5794  0.5801  0.5896
Median            0.5019    0.4558        0.5488        0.5067  0.4979  0.526
0.05 quantile     0.5578    0.5292        0.5869        0.5657  0.5508  0.5986
0.25 quantile     0.5435    0.5007        0.5869        0.5521  0.5362  0.5762
0.75 quantile     0.4777    0.6087        0.3445        0.4856  0.4641  0.5149
0.95 quantile     0.4573    0.5847        0.3277        0.4693  0.4370  0.5421

This shows that SD is most accurate, this means predictions of both control and schizophrenic.
SD is most sensitive, meaning predicted schizophrenic that actually were schizophrenic.
0.05 quantile and 0.25 quantile is the most specific, meaning predicted control that actually were control.
Range has the best PPV, meaning correctly predicted schizophrenic out of all schizophrenic.
SD has the best NPV, meaning correctly predicted controls out of all controls
SD has most area under the ROC cruve. This is an overall measure of how good the Sensitivity and Specificity are.

Best model to come

### Bonus question 5

You have some additional bonus data involving speech rate, pauses, etc. Include them in your analysis. Do they improve classification?

### Bonus question 6

Logistic regression is only one of many classification algorithms. Try using others and compare performance. Some examples: Discriminant Function, Random Forest, Support Vector Machine, etc. The package caret provides them.
