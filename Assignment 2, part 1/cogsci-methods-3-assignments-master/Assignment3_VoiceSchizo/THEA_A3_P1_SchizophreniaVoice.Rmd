---
title: "Assignment2_Part1_VoiceInSchizophrenia"
author: "Riccardo Fusaroli"
date: "July 17, 2016"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE}

library(tidyverse)
library(ggplot2)
library(lme4)
library(dplyr)
library(MuMIn)
library(effects)
library(Metrics)
library(caret)
library(simr)
library(lmerTest)
library(stringr)
library(purrr)
library(FinCal)

setwd("~/Studygroup/Assignment 2, part 1/cogsci-methods-3-assignments-master/Assignment3_VoiceSchizo/")
demo <- read.delim(file = "DemoData.txt", sep = "")

```

## Assignment 2 - Part 1 - Assessing voice in schizophrenia

Schizophrenia has been associated with "inappropriate" voice, sometimes monotone, sometimes croaky. A few studies indicate that pitch might be an index of schizophrenia. However, an ongoing meta-analysis of the literature (which you will have a go at in the last assignment) indicates that pitch mean and standard deviation are only weak indicators of diagnosis. Can we do better with our new fancy complex skills?

The corpus you are asked to analyse is a set of voice recordings from people with schizophrenia (just after first diagnosis) and 1-1 matched controls (on gender, age, education). Each participant watched 10 videos of triangles moving across the screen and had to describe them (so you have circa 10 recordings per person). We have already extracted the pitch once every 10 milliseconds and you will have to use this data to assess differences in the voice.

Can you characterize voice in schizophrenia as acoustically different? Report the methods you used to answer this question and the results from the analyses. Add a couple of lines trying to interpret the results (make sense of the difference). E.g. People with schizophrenia tend to have high-pitched voice, and present bigger swings in their prosody than controls. Add a couple of lines describing limitations of the data/analyses if any is relevant.


N.B. There are looots of files to be dealt with. Maybe too many for your computer, depending on how you load the files. This is a challenge for you. Some (complementary) possible strategies:
- You can select a subset of files only (and you have to justify your choice).
- You can learn how to use the apply() or map() functions.
- You can coordinate with classmates.

Hint: There is some information in the filenames that you might need.
Hint: It might be a good idea to first create a function which loads and parses one file, and then loop through the list of files to read them all. For example


#1. In the course of this assignment you have to first select one datafile and figure out how to:

- Extract "standard" descriptors of pitch: Mean, standard deviation, range
- Extract less "standard" descriptors of pitch you can think of (e.g. median, iqr, mean absoluted deviation, coefficient of variation)

#2. Second you will have to turn the code into a function and loop through all the files (or even better use apply/sapply/lapply)
- Remember to extract the relevant information from the file names (Participant, Diagnosis, Trial, Study)

```{r}

#define pathway to pitch folder
path = "/Users/thearolskovsloth/Studygroup/Assignment 2, part 1/cogsci-methods-3-assignments-master/Assignment3_VoiceSchizo/Pitch/"

#create function that extracts descriptors from pitch files
read_pitch <- function(filename) {
    # read data
  data <- read.delim(file = paste(path, filename, sep = ""), sep = "\t", header = TRUE) 
    # parse filename
  meta = str_extract_all(filename, "\\d+")
  study = as.numeric(meta[[1]][1])
  Diagnosis = as.numeric(meta[[1]][2])
  Subject = as.numeric(meta [[1]][3])
  triangles = as.numeric(meta[[1]][4])
  
    # extract descriptors
  mean_f0 = round(mean(data$f0), digits = 2)
  sd = sd(data$f0)
  quantiles = quantile(data$f0, probs = c(0.05, 0.25, 0.50, 0.75, 0.95))
  q05 = quantiles[[1]]
  q25 = quantiles[[2]]
  median = quantiles[[3]]
  q75 = quantiles[[4]]
  q95 = quantiles[[5]]
  iqr2575 = q75-q25
  coe.var = coefficient.variation(sd(data$f0), mean(data$f0))
  MAD = mad(data$f0) #average distance from something...
  
    # combine all this data
  output_list = data.frame(Subject, Diagnosis, study, triangles, mean_f0, sd, q05, q25, median, q75, q95, iqr2575, coe.var, MAD)
  
  return(output_list)
}


# test it on just one file while writing the function
test_data = read_pitch("Study1D0S101T1_f0.txt")

# run through all pitch data files
pitch_data = list.files(path = path) %>%
    purrr::map_df(read_pitch)

#write a csv file with combined data
write.csv(pitch_data, file = "THEA_pitch_data.csv")


```


```{r, include=FALSE}

#mutate new colum with subject id AND diagnosis to distinguish between subjects with and without schizophrenia
pitch_data <- pitch_data %>%
  mutate(Sub_Diag = str_c(Subject, Diagnosis, sep = ""))

#renamed diagnosis from demo-data
demo$Diagnosis <- ifelse(demo$Diagnosis == "Control", 0, 1)

demo <- demo %>%
  mutate(Sub_Diag = str_c(Subject, Diagnosis, sep = ""))

demo1 <- subset(demo, select = -c (SANS, SAPS, Social, NegLang, PosLang, Lang, Triangles, Alogia, FlatAffect, Asociality)) #excluding the columns that are not of use and full of NA's...


#merging datafiles in order to obtain the meta-data: gender, age, etc.
all_pitch <- full_join(demo1, pitch_data, by = "Sub_Diag")

#removing NA's
all_pitch <- na.omit(all_pitch)

all_pitch$Diagnosis.x <- as.factor(all_pitch$Diagnosis.x)

```


```{r, echo=TRUE}
#lets plot the data!
ggplot(all_pitch, aes(x= Diagnosis.x, y = Age, fill = Diagnosis.x)) +
  geom_boxplot() + 
  facet_wrap(~Study) + 
  scale_fill_manual(values=c("goldenrod1","dodgerblue")) +
  labs(x= "Study", y = "Age")

```


#3. Make one model per acoustic feature and test whether you can observe significant difference due to Diagnosis. Tip: Which other fixed factors should you control for (that is, include in the model)? Which random ones?
- Bonus points: cross-validate the model and report the betas and standard errors from all rounds to get an idea of how robust the estimates are. 

```{r, include=FALSE}

# mean_f0
mean_model <- lmer(mean_f0 ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(mean_model)

# sd 
sd_model <- lmer(sd ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(sd_model)

# q05 
q05_model <- lmer(q05 ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(q05_model)

# q25
q25_model <- lmer(q25 ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(q25_model)

# median
median_model <- lmer(median ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(median_model)

# q75 
q75_model <- lmer(q75 ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(q75_model)

# q95 
q95_model <- lmer(q95 ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(q95_model)

# iqr2575 = q75-q25
iqr2575_model <- lmer(iqr2575 ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(iqr2575_model)

# coe.var 
coe.var_model <- lmer(coe.var ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(coe.var_model)

# MAD 
MAD_model <- lmer(MAD ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(MAD_model)


```

#Cross validation 
```{r, include=FALSE}

#making the column numeric
all_pitch$Sub_Diag <- as.numeric(all_pitch$Sub_Diag)

#create empty lists
rmse_mean = NULL
rmse_sd = NULL
rmse_q05 = NULL
rmse_q25 = NULL
rmse_median = NULL
rmse_q75 = NULL
rmse_q95 = NULL
rmse_iqr2575 = NULL
rmse_coe.var = NULL
rmse_MAD = NULL

#create 5 folds 
folds <- createFolds(unique(all_pitch$Sub_Diag), k=5, list = TRUE, returnTrain = FALSE)
folds<-lapply(folds, function(x) unique(all_pitch$Sub_Diag)[x]) #to rename them what they are actually called in the data
folds


#create loop to cross validate the root mean square error of the models

for (f in folds) {
  #seperate in cake pieces
  temp_test = filter(all_pitch, Sub_Diag%in% f)
  temp_train = filter(all_pitch, !Sub_Diag%in% f)
  #other solution: data[f,] and data[!f,]
  
  #run training data through model
  mean_model <- lmer(mean_f0 ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), temp_train, REML = F)
  sd_model <- lmer(sd ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), temp_train, REML = F)
  q05_model <- lmer(q05 ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), temp_train, REML = F)
  q25_model <- lmer(q25 ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), temp_train, REML = F)
  median_model <- lmer(median ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), temp_train, REML = F)
  q75_model <- lmer(q75 ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), temp_train, REML = F)
  q95_model <- lmer(q95 ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), temp_train, REML = F)
  iqr2575_model <- lmer(iqr2575 ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), temp_train, REML = F)
  coe.var_model <- lmer(coe.var ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), temp_train, REML = F)
  MAD_model <- lmer(MAD ~ Diagnosis.x + Gender + (1| Sub_Diag) + (1| triangles), temp_train, REML = F)
  
  #predict test data from the two models
  pred_mean = predict(mean_model, temp_test, allow.new.levels =TRUE)
  pred_sd = predict(sd_model, temp_test, allow.new.levels =TRUE)
  pred_q05 = predict(q05_model, temp_test, allow.new.levels =TRUE)
  pred_q25 = predict(q25_model, temp_test, allow.new.levels =TRUE)
  pred_median = predict(median_model, temp_test, allow.new.levels =TRUE)
  pred_q75 = predict(q75_model, temp_test, allow.new.levels =TRUE)
  pred_q95 = predict(q95_model, temp_test, allow.new.levels =TRUE)
  pred_iqr2575 = predict(iqr2575_model, temp_test, allow.new.levels =TRUE)
  pred_coe.var = predict(coe.var_model, temp_test, allow.new.levels =TRUE)
  pred_MAD = predict(MAD_model, temp_test, allow.new.levels =TRUE)

  
  #get average error for prediction model and concatenate to the empty list
  rmse_mean = c(rmse_mean, rmse(temp_test$mean_f0, pred_mean))
  rmse_sd = c(rmse_sd, rmse(temp_test$sd, pred_sd))
  rmse_q05 = c(rmse_q05, rmse(temp_test$q05, pred_q05))
  rmse_q25 = c(rmse_q25, rmse(temp_test$q25, pred_q25))
  rmse_median <- c(rmse_median, rmse(temp_test$median, pred_median))
  rmse_q75 = c(rmse_q75, rmse(temp_test$q75, pred_q75))
  rmse_q95 = c(rmse_q95, rmse(temp_test$q95, pred_q95))
  rmse_iqr2575 = c(rmse_iqr2575, rmse(temp_test$iqr2575, pred_iqr2575))
  rmse_coe.var = c(rmse_coe.var, rmse(temp_test$coe.var, pred_coe.var))
  rmse_MAD = c(rmse_MAD, rmse(temp_test$MAD, pred_MAD))
  
#calculate the standardized value
  sd_rmse_mean = rmse_mean/sd(temp_test$mean_f0)
  sd_rmse_sd = rmse_sd/sd(temp_test$sd)
  sd_rmse_q05 = rmse_q05/sd(temp_test$q05)
  sd_rmse_q25 = rmse_q25/sd(temp_test$q25)
  sd_rmse_median = rmse_median/sd(temp_test$median)
  sd_rmse_q75 = rmse_q75/sd(temp_test$q75)
  sd_rmse_q95 = rmse_q95/sd(temp_test$q95)
  sd_rmse_iqr2575 = rmse_iqr2575/sd(temp_test$iqr2575)
  sd_rmse_coe.var = rmse_coe.var/sd(temp_test$coe.var)
  sd_rmse_MAD = rmse_MAD/sd(temp_test$MAD)
  
  }

#compare these numbers, as they indicate how well the model is trained to predict the test data
mean(rmse_mean)
mean(rmse_sd)
mean(rmse_q05)
mean(rmse_q25)
mean(rmse_median)
mean(rmse_q75)
mean(rmse_q95)
mean(rmse_iqr2575)
mean(rmse_coe.var)
mean(rmse_MAD)


#to get the standard
mean(sd_rmse_mean)
mean(sd_rmse_sd)
mean(sd_rmse_q05)
mean(sd_rmse_q25)
mean(sd_rmse_median)
mean(sd_rmse_q75)
mean(sd_rmse_q95)
mean(sd_rmse_iqr2575)
mean(sd_rmse_coe.var)
mean(sd_rmse_MAD)


```


#3a. Is study a significant predictor in these models? What should you infer from this? Does study interact with diagnosis? What should you infer from this?
```{r, include=FALSE}
#include "study"

# mean_f0
study_mean_model <- lmer(mean_f0 ~ Diagnosis.x + Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(study_mean_model)

# sd 
study_sd_model <- lmer(sd ~ Diagnosis.x + Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(study_sd_model)

# q05 
study_q05_model <- lmer(q05 ~ Diagnosis.x + Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(study_q05_model)

# q25
study_q25_model <- lmer(q25 ~ Diagnosis.x + Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(study_q25_model)

# median
study_median_model <- lmer(median ~ Diagnosis.x + Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(study_median_model)

# q75 
study_q75_model <- lmer(q75 ~ Diagnosis.x + Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(study_q75_model)

# q95 
study_q95_model <- lmer(q95 ~ Diagnosis.x + Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(study_q95_model)

# iqr2575 = q75-q25
study_iqr2575_model <- lmer(iqr2575 ~ Diagnosis.x + Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(study_iqr2575_model)

# coe.var 
study_coe.var_model <- lmer(coe.var ~ Diagnosis.x + Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(study_coe.var_model)

# MAD 
study_MAD_model <- lmer(MAD ~ Diagnosis.x + Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(study_MAD_model)



# INTERACTION OF STUDY #

# mean_f0
int_study_mean_model <- lmer(mean_f0 ~ Diagnosis.x*Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(int_study_mean_model)

# sd 
int_study_sd_model <- lmer(sd ~ Diagnosis.x*Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(int_study_sd_model)

# q05 
int_study_q05_model <- lmer(q05 ~ Diagnosis.x*Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(int_study_q05_model)

# q25
int_study_q25_model <- lmer(q25 ~ Diagnosis.x*Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(int_study_q25_model)

# median 
int_study_median_model <- lmer(median ~ Diagnosis.x*Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(int_study_median_model)

# q75 
int_study_q75_model <- lmer(q75 ~ Diagnosis.x*Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(int_study_q75_model)

# q95 
int_study_q95_model <- lmer(q95 ~ Diagnosis.x*Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(int_study_q95_model)

# iqr2575
int_study_iqr2575_model <- lmer(iqr2575 ~ Diagnosis.x*Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(int_study_iqr2575_model)


# coe.var 
int_study_coe.var_model <- lmer(coe.var ~ Diagnosis.x*Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(int_study_coe.var_model)

# MAD 
int_study_MAD_model <- lmer(MAD ~ Diagnosis.x*Study + Gender + (1| Sub_Diag) + (1| triangles), all_pitch, REML = F)
summary(int_study_MAD_model)



```

#6. Write a paragraph reporting methods and results

We created a model for each acoustic feature.
The standard model was: 

lmer(acoustic_feature ~ Diagnosis + Gender + (1| Subject) + (1| triangles))

These models showed that Diagnosis is a significant predictor for:

mean (β=13.36 (SE= 4.26), t = 3.14, p<.05)

q05 (β= 18.68 (SE= 4.55), t = 4.09, p<.05)

q25 (β= 16.40 (SE= 4.47), t = 3.67 , p<.05)

median (β= 11.29 (SE= 4.26), t = 2.65 , p<.05)

q75 (β= 10.98 (SE= 4.53), t = 2.42 , p<.05)

coe.var (β= -0.05 (SE= 0.01), t = -3.57 , p<.05)

MAD (β= -4.52 (SE= 1.28), t = -3.53 , p<.05)

Diagnosis was not a significant predictor for: sd, q95 and inter quantile range q75-q25.


The fixed effects was diagnosis as we wanted to test its axplanatory ability on pitch. Furthermore, Gender was included as a fixed effect. This was because we expect Gender to have a systematic influence on pitch, since males generally have a lower pitched voice than females.

The random effects was Subject as a random intercept, so we allow for subjects to have a different base level of pitch. Also, we included Triangles as a random intercept, to allow for the different videos of triangles to provoke different levels in the acoustic features.


To assess the different acoustic features we ran a 5 fold cross-validation of all the models. This was done in order to compare the predictive abilities of the different models with a root mean square error measure. 
The results of the cross validations were: 

acoustic feature      mean rmse value       standardized: mean rmse value/sd in percent

mean                  36.09                 63.21 %

sd                    25.97                 98.41 % 

q05                   38.17                 72.86 %

q25                   38.82                 71.35 %

median                33.92                 63.86 %

q75                   48.09                 68.95 %

q95                   70.31                 81.49 %

iqr2575               45.05                 93.39 %

coe.var               0.13                  107.92 %

MAD                   13.30                 109.12 %


The third column in the table is the calculated standardized measure of the models' predictive abilities. The calculations were done by taking the mean rmse value for the acoustic features and deviding them by the standard deviation of for that particular acoustic feature. This is done becasue we want to compare the error across different types of data with different units. Thereby, we get a standardized measure: the percentage from the standard deviation. Thus, the lower value, the better the model is at predicting. 

If we look at the rmse value for the median_model, then the rmse value is 33.92. In order to get the standardized measure we divide by the standard deviation of that acoustic feature (the variance around the mean). If the rmse value is lower than the sd value, we will get a standardized value below 1. If the sd value and the rmse value is the same, it would indicate that the error of the model is identical to the variance just present in the data. A good model should therefore have a lower error than the variance around the mean (sd). 

From this we can evaluate that the error is smallest in the models predicting median and mean.



When testing if study is a significant predictor, we included study as a fixed effect in the models.
Then the models looked like this:
lmer(acoustic_feature ~ Diagnosis + Study + Gender + (1| Sub_Diag) + (1| triangles))

None of these were significant. 

Neither was the interaction of study:
lmer(acoustic_feature ~ Diagnosis*Study + Gender + (1| Sub_Diag) + (1| triangles))

The fact that Study is not a significant predictor is a good thing. If it had in fact been signficant would have indicated that there had been some systematic difference in how the experiment was conducted across the studies. 
If there had been an interaction of study and diagnosis, it would have indicated that the study had a different effect on controls and schizophrenia patients. 

Maybe we should have checked assumptions?? Probably.........


## N.B. Remember to save the acoustic features of voice in a separate file, so to be able to load them next time